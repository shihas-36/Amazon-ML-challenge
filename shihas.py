# -*- coding: utf-8 -*-
"""shihas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VuH0Dp6dy8mt_fuAJH1FTj95m8BZ7fM_
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re # For regular expressions (used in later text cleaning/extraction)

# Set plotting style for better visualization
sns.set_style("whitegrid")

# Define file paths (assuming the script is run from a location where 'dataset/' is accessible)
TRAIN_FILE = '/content/drive/MyDrive/AmazonML/resource/student_resource/dataset/train.csv'
TEST_FILE = '/content/drive/MyDrive/AmazonML/resource/student_resource/dataset/test.csv'

print("--- A.1: Environment Setup and Data Loading ---")

# A.1.b: Load Data
try:
    # Load the training data
    train_df = pd.read_csv(TRAIN_FILE)
    print(f"Successfully loaded training data: {TRAIN_FILE}")
    print(f"Training data shape: {train_df.shape}")

    # Load the testing data
    test_df = pd.read_csv(TEST_FILE)
    print(f"Successfully loaded testing data: {TEST_FILE}")
    print(f"Testing data shape: {test_df.shape}")

except FileNotFoundError as e:
    print(f"Error: One or both files not found. Please ensure the 'dataset/' folder and files are correctly placed.")
    print(f"Missing file: {e.filename}")
    # Initialize empty dataframes to prevent errors in subsequent steps
    train_df = pd.DataFrame()
    test_df = pd.DataFrame()

# Display the first few rows and column information
if not train_df.empty:
    print("\n--- Training Data Head ---")
    print(train_df.head())
    print("\n--- Training Data Info ---")
    train_df.info()

print("\n--- A.1 Completed ---")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# File path assumption (change this if your file location is different)


# --- A.2.a: Distribution Check and Transformation ---
def check_price_distribution(file_path):
    try:
        train_df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: Training file not found at '{file_path}'. Please check the path.")
        return

    # 1. Create the log-transformed price column
    # np.log1p is np.log(1 + x), which stabilizes variance and handles prices of 0 gracefully.
    train_df['log_price'] = np.log1p(train_df['price'])

    print("\n--- Summary Statistics of Price and Log(1+Price) ---")
    print("Raw Price Statistics:")
    print(train_df['price'].describe())
    print("\nLog(1+Price) Statistics:")
    print(train_df['log_price'].describe())

    # 2. Visualize Distributions
    plt.figure(figsize=(14, 6))

    # Subplot 1: Raw Price Distribution (Filtering for better visualization of the bulk data)
    plt.subplot(1, 2, 1)
    # We plot prices up to the 99th percentile to prevent a few extreme outliers from flattening the histogram
    price_to_plot = train_df['price'][train_df['price'] < train_df['price'].quantile(0.99)]
    sns.histplot(price_to_plot, bins=50, kde=True, color='red')
    plt.title('Distribution of Raw Price (0-99th Percentile)', fontsize=14)
    plt.xlabel('Price', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)
    plt.ticklabel_format(style='plain', axis='x')

    # Subplot 2: Log(1+Price) Distribution
    plt.subplot(1, 2, 2)
    sns.histplot(train_df['log_price'], bins=50, kde=True, color='green')
    plt.title('Distribution of Log(1+Price) (Near Gaussian)', fontsize=14)
    plt.xlabel(r'$\log(1 + \text{Price})$', fontsize=12)
    plt.ylabel('Frequency', fontsize=12)

    plt.tight_layout()
    # Save the plot
    plot_filename = 'price_distribution_comparison.png'
    plt.savefig(plot_filename)
    print(f"\nSaved distribution comparison plot as {plot_filename}")

# Execute the function
check_price_distribution(TRAIN_FILE)

train_df

# Assuming 'train_df' is loaded and 'log_price' is already created (using np.log1p)
# We will use the 99.9th percentile of the log_price distribution as the cap.

# A.2.b: Outlier Handling
train_df['log_price'] = np.log1p(train_df['price'])
# 1. Define the upper percentile threshold (0.999 is 99.9th percentile)
HIGH_OUTLIER_THRESHOLD = 0.999

# Calculate the capping value based on the log-transformed price
cap_value = train_df['log_price'].quantile(HIGH_OUTLIER_THRESHOLD)

# 2. Cap the log_price values above the threshold
# We create a final, capped target column for training
train_df['log_price_capped'] = np.where(
    train_df['log_price'] > cap_value,
    cap_value,
    train_df['log_price']
)

# Convert the capping value back to the raw price scale for interpretation
raw_cap_price = np.expm1(cap_value) # expm1 is np.exp(x) - 1

print(f"\n--- A.2.b Outlier Handling Results ---")
print(f"99.9th percentile of log(1+price) (The Cap Value): {cap_value:.4f}")
print(f"This corresponds to a Raw Price Cap of: ${raw_cap_price:.2f}")
print(f"Number of prices capped (Top 0.1%): {(train_df['log_price'] > cap_value).sum()} / {len(train_df)}")

# Sanity Check: The number of capped values should be close to 0.1% of 75000 (i.e., around 75)
print("Target variable 'log_price_capped' is ready for modeling.")

import re

# Function to extract quantity and unit using regex
def extract_quantity_features(content):
    if pd.isna(content):
        return 0, 0, 0, 0 # Default values for missing content

    # Ensure content is a string and lowercase for uniform pattern matching
    text = str(content).lower()

    # 1. Quantity/Unit Extraction Patterns
    # Look for patterns like: 100g, 50ml, 1kg, 50pcs, 3pk, 10ct
    # Pattern: number + unit (g, ml, oz, kg, l, ct (count), pcs (pieces), pk (pack), sets, pairs)
    quantity_pattern = re.search(r'(\d+[\s\.]?\d*)\s*(g|ml|kg|l|oz|ct|pcs|pk|set|pair|m|ft|in|lb)', text)

    # 2. Pack/Count Extraction Patterns (e.g., "pack of 5", "x12")
    # Captures explicit counts without a size unit
    count_pattern_a = re.search(r'(pack|set|lot|pair)\s+of\s+(\d+)', text)
    count_pattern_b = re.search(r'x\s*(\d+)', text) # e.g., x12, x 12

    quantity_value = 0
    has_quantity_unit = 0

    if quantity_pattern:
        has_quantity_unit = 1
        # Extract the number (group 1) and unit (group 2)
        number_str = quantity_pattern.group(1).replace(' ', '')
        unit = quantity_pattern.group(2)

        try:
            quantity_value = float(number_str)
        except ValueError:
            pass

    # Overwrite/refine based on explicit count if quantity_value is still zero
    elif count_pattern_a:
        has_quantity_unit = 1
        try:
            quantity_value = float(count_pattern_a.group(2))
        except ValueError:
            pass
    elif count_pattern_b:
        has_quantity_unit = 1
        try:
            quantity_value = float(count_pattern_b.group(1))
        except ValueError:
            pass

    return (len(content), len(content.split()), has_quantity_unit, quantity_value)


# List of dataframes to process
datasets = [train_df, test_df]
dataset_names = ['Train', 'Test']

print("\n--- A.2.c: Initial Feature Engineering (Text) ---")

for df, name in zip(datasets, dataset_names):
    # Apply the feature extraction function
    df[['content_length', 'word_count', 'has_quantity_unit', 'extracted_quantity']] = \
        df['catalog_content'].apply(lambda x: pd.Series(extract_quantity_features(x)))

    # Standardize 'extracted_quantity': log transform it due to expected skewness
    # (A quantity of 1000 is very different from 1, so log scaling helps model this relationship)
    df['log_extracted_quantity'] = np.log1p(df['extracted_quantity'])

    # Display the results
    print(f"\n{name} Data - New Feature Statistics:")
    print(df[['content_length', 'word_count', 'has_quantity_unit', 'extracted_quantity', 'log_extracted_quantity']].describe())


# The dataframes (train_df, test_df) are now updated with the new features.
print("\n--- A.2.c Completed. Data is ready for Multimodal Feature Extraction (Part B) ---")

train_df



# After installation, we need to import the libraries
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from tqdm.autonotebook import tqdm
import torch # We need torch to check for CUDA availability

# --- DEVICE CHECK ---
# Determine the best available device: CUDA (GPU) if available, otherwise CPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"CUDA Check: {'Available' if DEVICE.type == 'cuda' else 'Not Available'}")
print(f"Model will be loaded onto device: {DEVICE}")

# 1. Model Selection (Efficient and compliant)
TRANSFORMER_MODEL_NAME = 'all-mpnet-base-v2'

print(f"--- B.1: Loading Transformer Model: {TRANSFORMER_MODEL_NAME} ---")

try:
    # Load the model directly to the determined DEVICE
    model = SentenceTransformer(TRANSFORMER_MODEL_NAME, device=DEVICE)
    MODEL_DIMENSION = model.get_sentence_embedding_dimension()
    print(f"Model loaded successfully. Output dimension: {MODEL_DIMENSION}D")
except Exception as e:
    print(f"FATAL ERROR: Could not load the model.")
    print(e)
    raise

# 2. Encoding Function
def generate_text_features(df):
    """Generates sentence embeddings for the catalog_content column with a progress bar."""

    # Fill any missing text with an empty string
    texts = df['catalog_content'].fillna('').tolist()

    print(f"\nGenerating {MODEL_DIMENSION}D embeddings for {len(texts)} samples on {DEVICE}...")

    # Encode texts. The encode function automatically uses the model's device.
    embeddings = model.encode(
        texts,
        batch_size=128, # Increased batch size for GPU efficiency
        show_progress_bar=True,
        convert_to_numpy=True
    )
    return embeddings

# 3. Execution (You must run this part after your dataframes are defined)
# NOTE: Requires train_df and test_df to be loaded from Part A.
train_text_features = generate_text_features(train_df)
test_text_features = generate_text_features(test_df)

print(f"\nText Feature Generation Complete.")
print(f"Train Text Features Shape: {train_text_features.shape}")
print(f"Test Text Features Shape: {test_text_features.shape}")



import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import os
import pandas as pd
from tqdm.autonotebook import tqdm # For tracking progress

# --- CONFIGURATION ---
IMAGE_SIZE = 224
IMAGE_DIMENSION = 2048 # ResNet50 output dimension after pooling
OUTPUT_DIR = 'product_images' # Local directory where images are stored
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"--- B.2: Using PyTorch for Visual Features on Device: {DEVICE} ---")

# 1. Image Preprocessing Pipeline
# Standard ImageNet normalization and resizing required by ResNet
preprocess = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 2. Load Pre-trained ResNet50 onto the CUDA device
base_model = models.resnet50(pretrained=True)
# Remove the final fully connected layer (the classifier)
feature_extractor = torch.nn.Sequential(*(list(base_model.children())[:-1]))
feature_extractor = feature_extractor.to(DEVICE)
feature_extractor.eval() # Set model to evaluation mode (crucial for inference)

print(f"ResNet50 loaded successfully. Output will be {IMAGE_DIMENSION}D vectors.")

import numpy as np
import pandas as pd # Needed to access the engineered columns

# 1. Define the Engineered Feature Columns
# These are the non-semantic, structured features created in A.2.c
engineered_cols = [
    'content_length',
    'word_count',
    'has_quantity_unit',
    'log_extracted_quantity' # The key engineered feature
]

print("--- B.3: Feature Consolidation ---")

# --- TRAIN DATA CONSOLIDATION ---
# Extract the engineered features as a NumPy array
train_engineered = train_df[engineered_cols].values

# Concatenate all three feature sets horizontally (axis=1)
# X_train = [Engineered | Text Embeddings | Image Features]
X_train = np.concatenate([
    train_engineered,
    train_text_features,
], axis=1)

# Define the final target variable (Y)
Y_train = train_df['log_price_capped'].values


# --- TEST DATA CONSOLIDATION ---
test_engineered = test_df[engineered_cols].values

# Concatenate the features for the test set
X_test = np.concatenate([
    test_engineered,
    test_text_features,
], axis=1)


# --- SUMMARY ---
total_features = X_train.shape[1]
print(f"Features Consolidated: {len(engineered_cols)} (Eng) + 384 (Text) + 2048 (Image)")
print(f"Total Dimensions per Sample: {total_features}")
print(f"X_train Shape (Features): {X_train.shape}")
print(f"Y_train Shape (Target): {Y_train.shape}")
print(f"X_test Shape (Features): {X_test.shape}")

print("\nMultimodal Feature Matrix is Ready for Modeling (Part C).")

# NOTE: Requires lightgbm and scikit-learn (pip install lightgbm scikit-learn)
import lightgbm as lgb
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm.autonotebook import tqdm # For tracking progress


X_train_split, X_val, Y_train_split, Y_val = train_test_split(
    X_train, Y_train, test_size=0.1, random_state=42
)

class TQDMCallback:
    def __init__(self, total_iterations):
        # Initialize tqdm progress bar
        self.pbar = tqdm(total=total_iterations, desc="LightGBM Training Progress")
        self.total_iterations = total_iterations
        self.best_score = float('inf') # Track best validation score
        self.best_iteration = -1

    def __call__(self, env):
        # env is a CallbackEnv object containing training information
        # env.iteration is the current boosting round (0-indexed)
        current_iter = env.iteration + 1
        self.pbar.update(1) # Increment the progress bar by one round

        # Optional: Update description with current validation loss
        if env.evaluation_result_list:
            # Assumes the first metric/set in the list is the validation RMSE
            # env.evaluation_result_list is a list of tuples: [(name, current_iter, score, is_higher_better)]
            current_score = env.evaluation_result_list[0][2]

            if current_score < self.best_score:
                self.best_score = current_score
                self.best_iteration = env.iteration

            self.pbar.set_description(
                f"LightGBM Training (Val RMSE: {current_score:.4f}, Best: {self.best_score:.4f})"
            )


        if env.model is not None and hasattr(env.model, 'best_iteration') and env.model.best_iteration != -1:

             pass # Let the main training loop finish and __del__ handle closing

        # Close the bar if we reached the total number of iterations
        if current_iter >= self.total_iterations:
             self.pbar.close()


    def __del__(self):
        # Ensure the bar is closed when the object is deleted
        if hasattr(self, 'pbar') and not self.pbar.closed:
             self.pbar.close()

# --- 3. Model Training ---

# Define total boosting rounds and Early Stopping rounds
N_ESTIMATORS = 1500
EARLY_STOPPING_ROUNDS = 50

lgb_params = {
    'objective': 'regression_l1', # MAE objective for robust error minimization
    'metric': 'rmse',             # Evaluation metric for early stopping
    'learning_rate': 0.03,
    'num_leaves': 40,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 1,
    'verbose': -1, # Important: disable LightGBM's internal verbose output
    'n_jobs': -1,
    'seed': 42,
    'lambda_l1': 0.1,  # L1 regularization on weights
    'lambda_l2': 0.1,  # L2 regularization on weights
    'min_gain_to_split': 0.01 # Minimum gain for a split to occur
}

print("Starting LightGBM Training (Multimodal Features)...")

# Initialize model using the core API (Dataset and train function)
lgb_train_data = lgb.Dataset(X_train_split, Y_train_split)
lgb_val_data = lgb.Dataset(X_val, Y_val, reference=lgb_train_data)

# Create the TQDM callback instance
# The total iterations for tqdm should match num_boost_round
tqdm_callback_instance = TQDMCallback(total_iterations=N_ESTIMATORS)


# Train the model
lgb_bst = lgb.train(
    params=lgb_params,
    train_set=lgb_train_data,
    num_boost_round=N_ESTIMATORS,
    valid_sets=[lgb_val_data], # Crucial for using the metric and early stopping
    callbacks=[
        lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False),
        tqdm_callback_instance # Insert our custom tqdm callback instance
    ]
)

# 4. Prediction (on the log scale)
# Predict using the best iteration found during early stopping
lgb_pred_log = lgb_bst.predict(X_test, num_iteration=lgb_bst.best_iteration)

print("\nLightGBM Training Complete.")
# lgb_bst.best_iteration is set by the early_stopping callback
print(f"Model used {lgb_bst.best_iteration + 1 if lgb_bst.best_iteration != -1 else N_ESTIMATORS} boosting rounds (best iteration: {lgb_bst.best_iteration}).")

# NOTE: Requires TensorFlow/Keras (pip install tensorflow)
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import numpy as np
from tensorflow.keras.regularizers import l2 # Import L2 regularizer

# Assuming X_train, Y_train, and X_test are ready from Part B.3

# --- 1. Data Preparation ---
# Split the training data to create a validation set for NN training
X_train_nn, X_val_nn, Y_train_nn, Y_val_nn = train_test_split(
    X_train, Y_train, test_size=0.15, random_state=42
)

# Ensure features are float32, which is required/preferred by NN accelerators
X_train_nn = X_train_nn.astype('float32')
X_val_nn = X_val_nn.astype('float32')
X_test_nn = X_test.astype('float32')

input_dim = X_train_nn.shape[1]

print("\n--- Part C.2: Defining Complex Multimodal MLP ---")

# --- 2. Architecture Definition ---

# Input Layer
input_tensor = Input(shape=(input_dim,), name='multimodal_input')

# Dense Block 1 with L2 regularization
x = Dense(1024, kernel_regularizer=l2(0.0001))(input_tensor) # Add L2 to weights
x = BatchNormalization()(x)
x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)
x = Dropout(0.3)(x)

# Dense Block 2 with L2 regularization
x = Dense(512, kernel_regularizer=l2(0.0001))(x) # Add L2 to weights
x = BatchNormalization()(x)
x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)
x = Dropout(0.3)(x)

# Dense Block 3 with L2 regularization
x = Dense(256, kernel_regularizer=l2(0.0001))(x) # Add L2 to weights
x = BatchNormalization()(x)
x = tf.keras.layers.LeakyReLU(alpha=0.01)(x)
x = Dropout(0.2)(x)

# Output Layer (1 neuron, linear activation for regression)
output_tensor = Dense(1, name='price_output')(x)

complex_nn_model = Model(inputs=input_tensor, outputs=output_tensor)

# 3. Compile the model
complex_nn_model.compile(optimizer=Adam(learning_rate=0.0005),
                         loss='mse',
                         metrics=['mae']) # Using MAE as an informative metric

# --- 4. Training Callbacks ---
callbacks = [
    # Stop training early if validation loss hasn't improved for 5 epochs
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    # Reduce learning rate when plateauing to encourage finer convergence
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)
]

# --- 5. Training ---
print("\nTraining Complex Multimodal MLP...")
history = complex_nn_model.fit(
    X_train_nn, Y_train_nn,
    epochs=50, # High max epoch, relying on EarlyStopping
    batch_size=512,
    validation_data=(X_val_nn, Y_val_nn), # Use the explicit validation set
    callbacks=callbacks,
    verbose=1
)

# --- 6. Prediction ---
nn_pred_log_complex = complex_nn_model.predict(X_test_nn, verbose=0).flatten()

print("Complex NN prediction on log scale complete.")

import numpy as np
import pandas as pd
import math # Not strictly necessary, but good for mathematical precision

# Assuming test_df, lgb_pred_log, and nn_pred_log_complex are available

# 1. Ensemble Predictions (Weighted Average)
# Start with a simple 50/50 average. You would tune these weights based on
# which model performed better on your validation set (e.g., lower validation SMAPE).
LGBM_WEIGHT = 0.5
NN_WEIGHT = 0.5

ensemble_pred_log = (lgb_pred_log * LGBM_WEIGHT) + (nn_pred_log_complex * NN_WEIGHT)

print(f"Ensembling predictions with weights: LGBM={LGBM_WEIGHT}, NN={NN_WEIGHT}")

# 2. Inverse Transformation (Crucial Step)
# Convert from log(1 + price) back to the raw price scale.
# np.expm1(x) is the inverse function: e^x - 1
final_predictions = np.expm1(ensemble_pred_log)

# 3. Constraint Handling (Must be Positive Floats)
# Ensure all predictions are positive floats (as required by the challenge).
MIN_PRICE_CONSTRAINT = 0.01
final_predictions[final_predictions <= 0] = MIN_PRICE_CONSTRAINT

# 4. Create the Submission DataFrame
submission_df = pd.DataFrame({
    'sample_id': test_df['sample_id'],
    # Round to two decimal places (standard for currency)
    'price': final_predictions.round(2)
})

# 5. Save to the required format
OUTPUT_FILE = 'test_out.csv'
submission_df.to_csv(OUTPUT_FILE, index=False)

print("\n--- Final Prediction Summary ---")
print(f"Final predictions saved to {OUTPUT_FILE}")
print(f"Output Shape: {submission_df.shape}")
print(f"Min Final Predicted Price: ${submission_df['price'].min():.2f}")
print(f"Max Final Predicted Price: ${submission_df['price'].max():.2f}")
print("\nChallenge Pipeline Complete! 🎉")

# To formally verify the generated file (assuming test_out.csv exists)

import pandas as pd

# Load the generated submission file
submission_check_df = pd.read_csv('test_out.csv')

print("\n--- Final Submission File Verification ---")
print("Headers and Dtypes:")
print(submission_check_df.info())

print("\nFirst 5 rows (Format check):")
print(submission_check_df.head())

# Final check of key constraints:
assert submission_check_df.shape[1] == 2, "Error: Must have exactly 2 columns."
assert all(col in submission_check_df.columns for col in ['sample_id', 'price']), "Error: Column names are incorrect."
assert submission_check_df['price'].min() >= 0.01, "Error: Price constraints violated (must be >= 0.01)."

print("\nVerification successful. The output format matches the required structure.")